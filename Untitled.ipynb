{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e466d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfMerger\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from chromadb import Client, Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "from openai import OpenAI  \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf30a1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFIG DATA\n",
    "root_dir = os.path.normpath(os.getcwd() + os.sep + os.pardir)\n",
    "Code_Path = os.path.abspath(os.curdir)\n",
    "Data_Path = os.path.join(root_dir,\"Data\")\n",
    "Input_Data = os.path.join(Data_Path,\"Input_PDF\")\n",
    "\n",
    "#ef = embedding_functions.ONNXMiniLM_L6_V2()\n",
    "#client = Client(settings = Settings(persist_directory=\"./\", is_persistent=True))\n",
    "#collection_ = client.get_or_create_collection(name=\"SinglePDFRead\", embedding_function=ef)\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8fc187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create chuncks to store the text into number of chunks\n",
    "\n",
    "def chunkstring(string: str, length: int):\n",
    "    return (string[0+i:length+i] for i in range(0, len(string), length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2de8095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(directory: str,length: int):\n",
    "        \n",
    "    # Initialize an empty dictionary to store the extracted text chunks\n",
    "    documents = {}\n",
    "    \n",
    "    for filenames in os.listdir(directory):  #List all the files\n",
    "        #iterate through each PDF Files\n",
    "        if filenames.lower().endswith('.pdf'):\n",
    "            #creating a pdf reader object\n",
    "            reader = PyPDF2.PdfReader(os.path.join(directory,filenames))\n",
    "                \n",
    "            # Iterate through each page in the PDF\n",
    "            for page_no in range(len(reader.pages)):\n",
    "                # get current page\n",
    "                page = reader.pages[page_no]\n",
    "                #Extract text from current page\n",
    "                texts = page.extract_text()\n",
    "        \n",
    "                #Preprocess the text \n",
    "\n",
    "                #Text = texts.lower()\n",
    "                #Removing Unicode character\n",
    "                #Text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", Text)\n",
    "                #Removing Stop Words\n",
    "                #stop = stopwords.words(\"english\")\n",
    "                #Text = \" \".join([word for word in Text.split() if word not in (stop)])\n",
    "        \n",
    "                #Convert Text into chunks\n",
    "                chunks = chunkstring(texts,length)\n",
    "        \n",
    "                # Store the text chunks in the documents dictionary with the page number as the key\n",
    "                documents[page_no] = chunks    \n",
    "    \n",
    "    # Return the dictionary containing page numbers as keys and text chunks as values\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43305aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(txt):\n",
    "    nlp_ob = nlp(txt)\n",
    "    #extracting tokens and storing it in list\n",
    "    tokens = [token.text for token in nlp_ob]\n",
    "    vectors = [token.vector for token in nlp_ob]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "54186537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_to_collection(directory: str,length: int):\n",
    "    #Load the pdf file and extract text into chunks\n",
    "    data = read_pdf(directory,length)\n",
    "    \n",
    "    embedding_data = {}\n",
    "    # Initialize empty lists to store data\n",
    "    docs_strings = []  # List to store text chunks\n",
    "    ids = []  # List to store unique IDs\n",
    "    metadatas = []  # List to store metadata for each text chunk\n",
    "    id = 0  # Initialize ID\n",
    "    \n",
    "   #iterate through each page and text chunk in pdf\n",
    "\n",
    "    for page_no in data.keys():\n",
    "        for doc in data[page_no]:\n",
    "            #append the text chunk in docs_strings\n",
    "            processed_doc = doc.replace(\"\\n\",\"\")\n",
    "            docs_strings.append(processed_doc)\n",
    "        \n",
    "            #Append meta data for text chunks in metadatas\n",
    "            metadatas.append({'Page no': page_no})\n",
    "        \n",
    "            #Append unique ids for the text chunks\n",
    "            ids.append(id)\n",
    "        \n",
    "            #increment the ids +1\n",
    "            id += 1\n",
    "    \n",
    "    \n",
    "    # Use TF-IDF vectorizer to convert text to vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    embedding_vectors = vectorizer.fit_transform(docs_strings)\n",
    "    \n",
    "    embedding_data['ids'] = ids\n",
    "    embedding_data['documents'] = docs_strings\n",
    "    embedding_data['metadatas'] = metadatas\n",
    "    embedding_data['Vectors'] = embedding_vectors\n",
    "    \n",
    "    return embedding_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "41914500",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_data = add_text_to_collection(Input_Data,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383df3c7",
   "metadata": {},
   "source": [
    "# GPT-2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "aa125610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_collection(question,answer,embedding_model):\n",
    "    embedded_data = add_text_to_collection(Input_Data,1000)\n",
    "    \n",
    "    # Use TF-IDF vectorizer to convert text to vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    question_vectors = vectorizer.fit_transform([question])\n",
    "    \n",
    "    answer_vectors = vectorizer.transform(answer)\n",
    "    \n",
    "    # Compute cosine similarity between the question and each answer\n",
    "    similarities = string_similarity(question, answer, embedding_model)\n",
    "    \n",
    "    # Get the index of the best answer\n",
    "    best_answer_index = np.argmax(similarities)\n",
    "    \n",
    "    return answer[best_answer_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c5da9ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Understanding Body Movement '"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_collection(\"What is Machine Learning\",embedded_data.get(\"documents\"),embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0d79f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open AI Model \n",
    "def get_response(queried_texts: list[str],):\n",
    "    global messages\n",
    "    client = OpenAI(api_key=\"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\")\n",
    "    \n",
    "    messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\\\n",
    "                 And will always answer the question asked in 'ques:' and \\\n",
    "                 will quote the page number while answering any questions,\\\n",
    "                 It is always at the start of the prompt in the format 'page n'.\"},\n",
    "                {\"role\": \"user\", \"content\": ''.join(queried_texts)}\n",
    "          ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                            model = \"gpt-3.5-turbo\",\n",
    "                            messages = messages,\n",
    "                            temperature=0.2,               \n",
    "                     )\n",
    "    response_msg = response.choices[0].message.content\n",
    "    messages = messages + [{\"role\":'assistant', 'content': response_msg}]\n",
    "    return response_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83c83d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(query: str):\n",
    "    queried_texts = query_collection(question = query,answer = embedded_data.get('documents'))\n",
    "    queried_string = [''.join(text) for text in queried_texts]\n",
    "    queried_string = queried_string[0] + f\"ques: {query}\"\n",
    "    answer = get_response(queried_texts = queried_string,)\n",
    "    ignore = ['Page','page']\n",
    "    final_answer = ' '.join(i for i in answer.split(' ') if not i.endswith(':') and i not in ignore)\n",
    "    return final_answer.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8188aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Yoga and different types\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b7607b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = get_answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301aca79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1cec14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
